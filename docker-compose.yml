version: "3.9"

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: my-ocr-api
    env_file:
      - ./app/.env
    environment:
      # Persist DB under a mounted dir
      DATABASE_URL: ${DATABASE_URL:-sqlite+aiosqlite:///./_data/data.db}
      # Point backend to OCR engine reachable in compose (adjust if you run engine elsewhere)
      LLM_BASE_URL: ${LLM_BASE_URL:-http://engine:8000/v1}
    ports:
      - "9000:9000"
    volumes:
      - db-data:/app/_data
    depends_on: []

  # Frontend: expects a Dockerfile under ./frontend to build the web app
  # Typical pattern (Vite): Node builder -> Nginx runner serving built assets
  web:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        # Common patterns, adjust to your frontend framework
        VITE_API_BASE_URL: http://api:9000
        NEXT_PUBLIC_API_BASE_URL: http://api:9000
    container_name: my-ocr-web
    environment:
      # Runtime env (if your frontend reads env at runtime via window.env or similar)
      VITE_API_BASE_URL: http://api:9000
      NEXT_PUBLIC_API_BASE_URL: http://api:9000
    ports:
      - "3000:80"
    depends_on:
      - api

  prometheus:
    image: prom/prometheus:latest
    container_name: my-ocr-prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-lifecycle"
    ports:
      - "9090:9090"
    volumes:
      - prometheus-data:/prometheus
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    depends_on:
      - api

  grafana:
    image: grafana/grafana:latest
    container_name: my-ocr-grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
    ports:
      - "3001:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus

  # Optional OCR engine (OpenAI-compatible) if you want to run it inside compose
  # Replace image/command/env according to your engine setup; otherwise, point
  # LLM_BASE_URL in the api service to an external endpoint.
  # engine:
  #   image: your-ocr-engine:latest
  #   container_name: my-ocr-engine
  #   ports:
  #     - "8000:8000"
  #   environment:
  #     - SOME_ENGINE_ENV=...

volumes:
  db-data:
  prometheus-data:
  grafana-data:
