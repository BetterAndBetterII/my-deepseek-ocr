version: "3.9"

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: my-ocr-api
    env_file:
      - ./app/.env
    environment:
      # Persist DB under a mounted dir
      DATABASE_URL: ${DATABASE_URL:-sqlite+aiosqlite:///./_data/data.db}
      # Point backend to OCR engine reachable in compose (adjust if you run engine elsewhere)
      LLM_BASE_URL: ${LLM_BASE_URL:-http://engine:8000/v1}
    volumes:
      - db-data:/app/_data
    depends_on:
      - engine

  # Frontend: expects a Dockerfile under ./frontend to build the web app
  # Typical pattern (Vite): Node builder -> Nginx runner serving built assets
  web:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: my-ocr-web
    environment:
      BACKEND_URL: http://api:8000
    ports:
      - "3000:80"
    depends_on:
      - api

  prometheus:
    image: prom/prometheus:latest
    container_name: my-ocr-prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-lifecycle"
    ports:
      - "9090:9090"
    volumes:
      - prometheus-data:/prometheus
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    depends_on:
      - api

  grafana:
    image: grafana/grafana:latest
    container_name: my-ocr-grafana
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH=/etc/grafana/provisioning/dashboards/compose-jobs-overview.json
    ports:
      - "3001:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus

  engine:
    image: vllm/vllm-openai:nightly
    container_name: my-ocr-engine
    restart: unless-stopped
    gpus: "all"
    ipc: "host"
    shm_size: "16g"
    ports:
      - "8000:8000"
    environment:
      - NVIDIA_VISIBLE_DEVICES=0,1
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
      - ${HOME}/vllm:/root/vllm
    command:
      - --model
      - deepseek-ai/DeepSeek-OCR
      - --tensor-parallel-size
      - "2"
      - --distributed-executor-backend
      - mp
      - --logits-processors
      - "vllm.model_executor.models.deepseek_ocr:NGramPerReqLogitsProcessor"
      - --chat-template
      - /root/vllm/template_deepseek_ocr.jinja

volumes:
  db-data:
  prometheus-data:
  grafana-data:
